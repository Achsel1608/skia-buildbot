# DO NOT EDIT: This file is generated by running the 'absent' tool.

ALERT MissingData
  IF absent(up)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "up",
    description = "There is no data for the following alert: up"
  }

ALERT MissingData
  IF absent(go_goroutines)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "go_goroutines",
    description = "There is no data for the following alert: go_goroutines"
  }

ALERT MissingData
  IF absent(prober{type="failure"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "prober{type=\"failure\"}",
    description = "There is no data for the following alert: prober{type=\"failure\"}"
  }

ALERT MissingData
  IF absent(reboot_required_i)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "reboot_required_i",
    description = "There is no data for the following alert: reboot_required_i"
  }

ALERT MissingData
  IF absent(df_complex_free{resource="df-root",host!~".*rpi-.+"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "df_complex_free{resource=\"df-root\",host!~\".*rpi-.+\"}",
    description = "There is no data for the following alert: df_complex_free{resource=\"df-root\",host!~\".*rpi-.+\"}"
  }

ALERT MissingData
  IF absent(df_complex_free{resource=~"df-mnt-pd0|df-mnt-dds0"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "df_complex_free{resource=~\"df-mnt-pd0|df-mnt-dds0\"}",
    description = "There is no data for the following alert: df_complex_free{resource=~\"df-mnt-pd0|df-mnt-dds0\"}"
  }

ALERT MissingData
  IF absent(df_complex_free{resource="df-b",host!~".*rpi-.+"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "df_complex_free{resource=\"df-b\",host!~\".*rpi-.+\"}",
    description = "There is no data for the following alert: df_complex_free{resource=\"df-b\",host!~\".*rpi-.+\"}"
  }

ALERT MissingData
  IF absent(df_complex_free{resource=~"df-var|df-tmp",host=~"skia-rpi-.+"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "df_complex_free{resource=~\"df-var|df-tmp\",host=~\"skia-rpi-.+\"}",
    description = "There is no data for the following alert: df_complex_free{resource=~\"df-var|df-tmp\",host=~\"skia-rpi-.+\"}"
  }

ALERT MissingData
  IF absent(avg_over_time(dirty_packages[25h]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "avg_over_time(dirty_packages[25h])",
    description = "There is no data for the following alert: avg_over_time(dirty_packages[25h])"
  }

ALERT MissingData
  IF absent(rate(pulld_failed_install[10m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(pulld_failed_install[10m])",
    description = "There is no data for the following alert: rate(pulld_failed_install[10m])"
  }

ALERT MissingData
  IF absent(healthy{instance="skia-ct-master:20000",job="ct-poller"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "healthy{instance=\"skia-ct-master:20000\",job=\"ct-poller\"}",
    description = "There is no data for the following alert: healthy{instance=\"skia-ct-master:20000\",job=\"ct-poller\"}"
  }

ALERT MissingData
  IF absent(num_pending_tasks{instance="skia-ctfe:20000",job="ctfe"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "num_pending_tasks{instance=\"skia-ctfe:20000\",job=\"ctfe\"}",
    description = "There is no data for the following alert: num_pending_tasks{instance=\"skia-ctfe:20000\",job=\"ctfe\"}"
  }

ALERT MissingData
  IF absent(oldest_pending_task_status{instance="skia-ctfe:20000",job="ctfe"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "oldest_pending_task_status{instance=\"skia-ctfe:20000\",job=\"ctfe\"}",
    description = "There is no data for the following alert: oldest_pending_task_status{instance=\"skia-ctfe:20000\",job=\"ctfe\"}"
  }

ALERT MissingData
  IF absent(rate(num_log_lines{level="ERROR",log_source="ctfe"}[2m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(num_log_lines{level=\"ERROR\",log_source=\"ctfe\"}[2m])",
    description = "There is no data for the following alert: rate(num_log_lines{level=\"ERROR\",log_source=\"ctfe\"}[2m])"
  }

ALERT MissingData
  IF absent(cq_watcher_in_flight_waiting_in_cq{instance="skia-cq-watcher:20000",job="cq_watcher"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "cq_watcher_in_flight_waiting_in_cq{instance=\"skia-cq-watcher:20000\",job=\"cq_watcher\"}",
    description = "There is no data for the following alert: cq_watcher_in_flight_waiting_in_cq{instance=\"skia-cq-watcher:20000\",job=\"cq_watcher\"}"
  }

ALERT MissingData
  IF absent(max_over_time(cq_watcher_in_flight_trybot_duration{instance="skia-cq-watcher:20000",job="cq_watcher"}[20m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "max_over_time(cq_watcher_in_flight_trybot_duration{instance=\"skia-cq-watcher:20000\",job=\"cq_watcher\"}[20m])",
    description = "There is no data for the following alert: max_over_time(cq_watcher_in_flight_trybot_duration{instance=\"skia-cq-watcher:20000\",job=\"cq_watcher\"}[20m])"
  }

ALERT MissingData
  IF absent(max_over_time(cq_watcher_in_flight_trybot_num{instance="skia-cq-watcher:20000",job="cq_watcher"}[20m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "max_over_time(cq_watcher_in_flight_trybot_num{instance=\"skia-cq-watcher:20000\",job=\"cq_watcher\"}[20m])",
    description = "There is no data for the following alert: max_over_time(cq_watcher_in_flight_trybot_num{instance=\"skia-cq-watcher:20000\",job=\"cq_watcher\"}[20m])"
  }

ALERT MissingData
  IF absent(perf_clustering_untriaged{instance="skia-perf:20000"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "perf_clustering_untriaged{instance=\"skia-perf:20000\"}",
    description = "There is no data for the following alert: perf_clustering_untriaged{instance=\"skia-perf:20000\"}"
  }

ALERT MissingData
  IF absent(perf_clustering_untriaged{instance=~"skia-android.*:20000"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "perf_clustering_untriaged{instance=~\"skia-android.*:20000\"}",
    description = "There is no data for the following alert: perf_clustering_untriaged{instance=~\"skia-android.*:20000\"}"
  }

ALERT MissingData
  IF absent(rate(process_failures[2m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(process_failures[2m])",
    description = "There is no data for the following alert: rate(process_failures[2m])"
  }

ALERT MissingData
  IF absent(rate(ingestion{metric="errors"}[5m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(ingestion{metric=\"errors\"}[5m])",
    description = "There is no data for the following alert: rate(ingestion{metric=\"errors\"}[5m])"
  }

ALERT MissingData
  IF absent(builds_failed)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "builds_failed",
    description = "There is no data for the following alert: builds_failed"
  }

ALERT MissingData
  IF absent(repo_sync_failed)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "repo_sync_failed",
    description = "There is no data for the following alert: repo_sync_failed"
  }

ALERT MissingData
  IF absent(named_failures)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "named_failures",
    description = "There is no data for the following alert: named_failures"
  }

ALERT MissingData
  IF absent(datahopper_internal_ingest_build_webhook_oldest_untested_commit_age/60/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "datahopper_internal_ingest_build_webhook_oldest_untested_commit_age/60/60",
    description = "There is no data for the following alert: datahopper_internal_ingest_build_webhook_oldest_untested_commit_age/60/60"
  }

ALERT MissingData
  IF absent(liveness_ingest_build_webhook_oldest_untested_commit_age_metric_s/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_ingest_build_webhook_oldest_untested_commit_age_metric_s/60",
    description = "There is no data for the following alert: liveness_ingest_build_webhook_oldest_untested_commit_age_metric_s/60"
  }

ALERT MissingData
  IF absent(go_goroutines{instance="skia-internal:20000",job="datahopper_internal"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "go_goroutines{instance=\"skia-internal:20000\",job=\"datahopper_internal\"}",
    description = "There is no data for the following alert: go_goroutines{instance=\"skia-internal:20000\",job=\"datahopper_internal\"}"
  }

ALERT MissingData
  IF absent(rate(num_log_lines{level="ERROR",log_source="datahopper"}[10m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(num_log_lines{level=\"ERROR\",log_source=\"datahopper\"}[10m])",
    description = "There is no data for the following alert: rate(num_log_lines{level=\"ERROR\",log_source=\"datahopper\"}[10m])"
  }

ALERT MissingData
  IF absent(swarming_bots_last_seen/1024/1024/1024/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "swarming_bots_last_seen/1024/1024/1024/60",
    description = "There is no data for the following alert: swarming_bots_last_seen/1024/1024/1024/60"
  }

ALERT MissingData
  IF absent(avg_over_time(swarming_bots_quarantined[10m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "avg_over_time(swarming_bots_quarantined[10m])",
    description = "There is no data for the following alert: avg_over_time(swarming_bots_quarantined[10m])"
  }

ALERT MissingData
  IF absent(fuzzer_queue_size_upload)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "fuzzer_queue_size_upload",
    description = "There is no data for the following alert: fuzzer_queue_size_upload"
  }

ALERT MissingData
  IF absent(fuzzer_queue_size_analysis)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "fuzzer_queue_size_analysis",
    description = "There is no data for the following alert: fuzzer_queue_size_analysis"
  }

ALERT MissingData
  IF absent(fuzzer_version_age{type="current"}/60/60/24)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "fuzzer_version_age{type=\"current\"}/60/60/24",
    description = "There is no data for the following alert: fuzzer_version_age{type=\"current\"}/60/60/24"
  }

ALERT MissingData
  IF absent(fuzzer_version_age{type="pending"}/60/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "fuzzer_version_age{type=\"pending\"}/60/60",
    description = "There is no data for the following alert: fuzzer_version_age{type=\"pending\"}/60/60"
  }

ALERT MissingData
  IF absent(avg_over_time(prober{probename="skiastatus_json",type="latency"}[10m])/1024)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "avg_over_time(prober{probename=\"skiastatus_json\",type=\"latency\"}[10m])/1024",
    description = "There is no data for the following alert: avg_over_time(prober{probename=\"skiastatus_json\",type=\"latency\"}[10m])/1024"
  }

ALERT MissingData
  IF absent(liveness_probes_s/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_probes_s/60",
    description = "There is no data for the following alert: liveness_probes_s/60"
  }

ALERT MissingData
  IF absent(liveness_issue_tracker_s/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_issue_tracker_s/60",
    description = "There is no data for the following alert: liveness_issue_tracker_s/60"
  }

ALERT MissingData
  IF absent(autoroll_last_roll_result{child_path="src/third_party/skia"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "autoroll_last_roll_result{child_path=\"src/third_party/skia\"}",
    description = "There is no data for the following alert: autoroll_last_roll_result{child_path=\"src/third_party/skia\"}"
  }

ALERT MissingData
  IF absent(liveness_last_autoroll_landed_s{child_path="src/third_party/skia"}/60/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_last_autoroll_landed_s{child_path=\"src/third_party/skia\"}/60/60",
    description = "There is no data for the following alert: liveness_last_autoroll_landed_s{child_path=\"src/third_party/skia\"}/60/60"
  }

ALERT MissingData
  IF absent(prober{type="latency",probename="autoroll"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "prober{type=\"latency\",probename=\"autoroll\"}",
    description = "There is no data for the following alert: prober{type=\"latency\",probename=\"autoroll\"}"
  }

ALERT MissingData
  IF absent(rate(num_log_lines{level="ERROR",log_source="autoroll"}[10m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(num_log_lines{level=\"ERROR\",log_source=\"autoroll\"}[10m])",
    description = "There is no data for the following alert: rate(num_log_lines{level=\"ERROR\",log_source=\"autoroll\"}[10m])"
  }

ALERT MissingData
  IF absent(gold_status_by_corpus{type="untriaged",instance="skia-gold-prod:20001"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "gold_status_by_corpus{type=\"untriaged\",instance=\"skia-gold-prod:20001\"}",
    description = "There is no data for the following alert: gold_status_by_corpus{type=\"untriaged\",instance=\"skia-gold-prod:20001\"}"
  }

ALERT MissingData
  IF absent(gold_num_expired_ignore_rules{instance="skia-gold-prod:20001"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "gold_num_expired_ignore_rules{instance=\"skia-gold-prod:20001\"}",
    description = "There is no data for the following alert: gold_num_expired_ignore_rules{instance=\"skia-gold-prod:20001\"}"
  }

ALERT MissingData
  IF absent(liveness_gold_expired_ignore_rules_monitoring_s{instance="skia-gold-prod:20001"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_gold_expired_ignore_rules_monitoring_s{instance=\"skia-gold-prod:20001\"}",
    description = "There is no data for the following alert: liveness_gold_expired_ignore_rules_monitoring_s{instance=\"skia-gold-prod:20001\"}"
  }

ALERT MissingData
  IF absent(rate(num_log_lines{level="ERROR",job="gold"}[2m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(num_log_lines{level=\"ERROR\",job=\"gold\"}[2m])",
    description = "There is no data for the following alert: rate(num_log_lines{level=\"ERROR\",job=\"gold\"}[2m])"
  }

ALERT MissingData
  IF absent(liveness_gold_s{metric="since-last-run",source="poll"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_gold_s{metric=\"since-last-run\",source=\"poll\"}",
    description = "There is no data for the following alert: liveness_gold_s{metric=\"since-last-run\",source=\"poll\"}"
  }

ALERT MissingData
  IF absent(rate(num_log_lines{level="ERROR",job="ingestion"}[2m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(num_log_lines{level=\"ERROR\",job=\"ingestion\"}[2m])",
    description = "There is no data for the following alert: rate(num_log_lines{level=\"ERROR\",job=\"ingestion\"}[2m])"
  }

ALERT MissingData
  IF absent(liveness_last_successful_task_scheduling_s/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_last_successful_task_scheduling_s/60",
    description = "There is no data for the following alert: liveness_last_successful_task_scheduling_s/60"
  }

ALERT MissingData
  IF absent(prober{type="latency",probename="task_scheduler"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "prober{type=\"latency\",probename=\"task_scheduler\"}",
    description = "There is no data for the following alert: prober{type=\"latency\",probename=\"task_scheduler\"}"
  }

ALERT MissingData
  IF absent(rate(num_log_lines{level="ERROR",log_source="task_scheduler"}[2m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "rate(num_log_lines{level=\"ERROR\",log_source=\"task_scheduler\"}[2m])",
    description = "There is no data for the following alert: rate(num_log_lines{level=\"ERROR\",log_source=\"task_scheduler\"}[2m])"
  }

ALERT MissingData
  IF absent(liveness_last_db_backup_s/60/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_last_db_backup_s/60/60",
    description = "There is no data for the following alert: liveness_last_db_backup_s/60/60"
  }

ALERT MissingData
  IF absent(recent_db_backup_count)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "recent_db_backup_count",
    description = "There is no data for the following alert: recent_db_backup_count"
  }

ALERT MissingData
  IF absent(liveness_db_backup_maybe_backup_db_s/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_db_backup_maybe_backup_db_s/60",
    description = "There is no data for the following alert: liveness_db_backup_maybe_backup_db_s/60"
  }

ALERT MissingData
  IF absent(liveness_incremental_backup_s/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_incremental_backup_s/60",
    description = "There is no data for the following alert: liveness_incremental_backup_s/60"
  }

ALERT MissingData
  IF absent(incremental_backup_reset)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "incremental_backup_reset",
    description = "There is no data for the following alert: incremental_backup_reset"
  }

ALERT MissingData
  IF absent(bolt_db{metric="FreePageCount",database="task_scheduler_db"})
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "bolt_db{metric=\"FreePageCount\",database=\"task_scheduler_db\"}",
    description = "There is no data for the following alert: bolt_db{metric=\"FreePageCount\",database=\"task_scheduler_db\"}"
  }

ALERT MissingData
  IF absent(liveness_task_scheduler_periodic_trigger_s{trigger="nightly"}/60/60)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_task_scheduler_periodic_trigger_s{trigger=\"nightly\"}/60/60",
    description = "There is no data for the following alert: liveness_task_scheduler_periodic_trigger_s{trigger=\"nightly\"}/60/60"
  }

ALERT MissingData
  IF absent(liveness_task_scheduler_periodic_trigger_s{trigger="weekly"}/60/60/24)
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "liveness_task_scheduler_periodic_trigger_s{trigger=\"weekly\"}/60/60/24",
    description = "There is no data for the following alert: liveness_task_scheduler_periodic_trigger_s{trigger=\"weekly\"}/60/60/24"
  }

ALERT MissingData
  IF absent(avg_over_time(skolo_router_backup_backup_size[25h]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "avg_over_time(skolo_router_backup_backup_size[25h])",
    description = "There is no data for the following alert: avg_over_time(skolo_router_backup_backup_size[25h])"
  }

ALERT MissingData
  IF absent(avg_over_time(skolo_rpi_backup_backup_size[25h]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "avg_over_time(skolo_rpi_backup_backup_size[25h])",
    description = "There is no data for the following alert: avg_over_time(skolo_rpi_backup_backup_size[25h])"
  }

ALERT MissingData
  IF absent(max_over_time(skolo_hotspare_consecutive_failures[10m]))
  FOR 5m
  LABELS { category = "infra", severity = "critical" }
  ANNOTATIONS {
    abbr = "max_over_time(skolo_hotspare_consecutive_failures[10m])",
    description = "There is no data for the following alert: max_over_time(skolo_hotspare_consecutive_failures[10m])"
  }

