# This file defines alerts to be triggered by the server.

#
# AlertServer should tolerate no errors.


[[rule]]
name = "AlertServer Errors"
message = "The Error rate for the alertserver is too high, please check the logs."
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='alertserver' AND level='ERROR' AND host='skia-alerts' order by time desc limit 1"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"


#
# General
#

[[rule]]
name = "Too many Go routines in %(app)s"
message = "Too many Go routines in %(app)s running on %(host)s"
database = "skmetrics"
query = "SELECT mean(value) FROM \"runtime-metrics\" WHERE metric='num-goroutine' AND time > now() - 10m GROUP BY app,host"
category = "infra"
conditions = ["x > 3000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Root Disk Space on %(host)s"
message = "Free space has fallen below 1GB on %(host)s root drive."
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE resource = 'df-root' AND host !~ /skia-.*rpi-.+/ AND time > now() - 5m GROUP BY host"
category = "infra"
conditions = ["x <= 1e9"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Disk Space on %(host)s"
message = "Free space has fallen below 10GB on %(host)s drive %(resource)s."
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE time > now() - 5m AND resource = 'df-mnt-pd0' OR resource = 'df-mnt-ssd0' GROUP BY host,resource"
category = "infra"
conditions = ["x <= 1e10"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Disk Space on VM %(host)s"
message = "Free space has fallen below 20GB on %(host)s drive %(resource)s.\n\nTry running:\ngo run scripts/run_on_swarming_bots/run_on_swarming_bots.go --alsologtostderr --script=scripts/run_on_swarming_bots/delete_out_dirs.py --dimension id:%(host)s"
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE time > now() - 5m AND resource = 'df-b' AND host !~ /skia-rpi-\\d+/ GROUP BY host,resource"
category = "infra"
conditions = ["x <= 2e10"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Disk Space on Skolo host %(host)s"
message = "Free space has fallen below 100MB on %(host)s drive %(resource)s."
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE time > now() - 5m AND host =~ /skia-rpi-.+/ AND resource = 'df-var' OR resource = 'df-tmp' GROUP BY host,resource"
category = "infra"
conditions = ["x <= 1e8"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Probe failed %(probename)s"
message = "Endpoint %(probename)s has failed to respond in at least 10 minutes. See https://github.com/google/skia-buildbot/search?q=%(probename)s+filename%3Aprobers.json for the endpoint URL."
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='failure' group by probename;"
category = "infra"
conditions = ["x >= 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Instance %(host)s needs rebooting.  Owner(s): %(owners)s"
message = "See https://mon.skia.org/dashboard/db/reboots-required for the full list of instances that need rebooting."
database = "skmetrics"
query = "select mean(value) from \"reboot-required-i\" where time > now() - 5m AND app='pulld' group by host,owners;"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Skolo instance %(host)s needs rebooting.  Owner(s): kjlubick, stephana"
message = "See https://mon.skia.org/dashboard/db/reboots-required for the full list of instances that need rebooting."
database = "skmetrics"
query = "select mean(value) from \"reboot-required-i\" where time > now() - 5m AND app='pulld-not-gce' group by host;"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# SkiaGold
#

[[rule]]
name = "Gold Alert (GM)"
message = "At least one untriaged GM has been found. Please visit https://gold.skia.org/ to triage."
database = "skmetrics"
query = "select value from \"gold.status.by-corpus\" WHERE time > now() - 10m AND type='untriaged' AND app = 'skiacorrectness' AND host = 'skia-gold-prod' order by time desc limit 1"
category = "Gold"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"

[[rule]]
name = "Expired Ignores (Gold)"
message = "At least one expired ignore rule has been found. Please visit https://gold.skia.org/ignores to delete or extend."
database = "skmetrics"
query = "select value from \"gold.num-expired-ignore-rules\" WHERE time > now() - 10m AND app = 'skiacorrectness' AND host = 'skia-gold-prod' order by time desc limit 1"
category = "Gold"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"

[[rule]]
name = "Ingestion Failure (Gold)"
message = """At least two rounds of gold ingestion have failed back to back.

See: https://mon.skia.org/dashboard/db/ingestion
"""
database = "skmetrics"
query = "SELECT mean(value) FROM liveness WHERE time > now() - 10m AND app='gold_ingestion' AND \"name\"='gold' AND source='poll' AND metric='since-last-run'"
category = "infra"
conditions = ["x >= 750"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Ignore Monitoring Failure (Gold)"
message = "At least two rounds of monitoring for expired ignore rules have failed back to back."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\"='gold.expired-ignore-rules-monitoring' AND app='skiacorrectness' AND host='skia-gold-prod'"
category = "infra"
conditions = ["x >= 200"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Gold Error Rate"
message = "The error rate for Gold is too high."
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" WHERE time > now() - 10m AND \"name\"='skiacorrectness' AND level='ERROR' AND host='skia-gold-prod' order by time desc limit 1"
category = "infra"
conditions = ["x >= 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "Gold Ingestion Error Rate"
message = """The error rate for Gold Ingestion is too high.

See: https://mon.skia.org/dashboard/db/ingestion
"""
database = "skmetrics"
query = "SELECT  MAX(value) FROM ingestion WHERE time > now() - 10m AND ingester='gold' AND \"metric\"='errors' AND source='poll'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Gold Trybot Ingestion Error Rate"
message = """The error rate for Gold Trybot Ingestion is too high.

See: https://mon.skia.org/dashboard/db/ingestion
"""
database = "skmetrics"
query = "SELECT  MAX(value) FROM ingestion WHERE time > now() - 10m AND ingester='gold-trybot' AND \"metric\"='errors' AND source='poll'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "PDF Ingestion Error Rate"
message = "The error rate for PDF Ingestion is too high."
database = "skmetrics"
query = "SELECT  MAX(value) FROM ingestion WHERE time > now() - 30m AND ingester='pdf' AND \"metric\"='errors' AND source='poll'"
category = "infra"
conditions = ["x >= 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

#
# Skia Status
#

[[rule]]
name = "Skia Status Latency (JSON endpoint)"
message = "The JSON endpoint at https://status.skia.org/json/skia/commits/ took more than 300ms to respond."
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename='skiastatus_json'"
category = "infra"
conditions = ["x / 1000000 > 300"]
actions = ["Email(infra-alerts@skia.org)", "Email(borenet@google.com)"]
auto-dismiss = true
nag = "1h"

#
# Skia Alerts
#

[[rule]]
name = "Skia Alerts Latency (JSON endpoint)"
message = "The JSON endpoint at https://alerts.skia.org/json/alerts/ took more than 200ms to respond."
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename='skiaalerts_json'"
category = "infra"
conditions = ["x / 1000000 > 200"]
actions = ["Email(infra-alerts@skia.org)", "Email(borenet@google.com)"]
auto-dismiss = true
nag = "1h"

#
# Skia AutoRoll Bot
#

[[rule]]
name = "Skia AutoRoll Failed"
message = "The last DEPS roll attempt for Skia failed. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#autoroll_failed"
database = "skmetrics"
query = "select value from \"autoroll.last-roll-result\" where time > now() - 10m AND \"child-path\"='src/third_party/skia' order by time desc limit 1"
category = "Skia"
conditions = ["x == 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "2h"

[[rule]]
name = "No AutoRolls Landed for 24h (Skia)"
message = "The last-landed AutoRoll for Skia was over 24h ago. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#no_rolls_24h"
database = "skmetrics"
query = "select value from \"liveness\" where time > now() - 10m AND \"name\"='last-autoroll-landed' AND \"child-path\"='src/third_party/skia' order by time desc limit 1"
category = "Skia"
conditions = ["x > 24 * 60 * 60"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "2h"

[[rule]]
name = "AutoRoll HTTP Latency (%(probename)s)"
message = "The endpoint for %(probename)s took more than 200ms to respond. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#http_latency"
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename =~ /autoroll/ group by probename"
category = "infra"
conditions = ["x / 1000000 > 200"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "AutoRoll Error Rate (%(host)s)"
message = "The error rate for autoroll on %(host)s is too high. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#error_rate"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='autoroll' AND level='ERROR' group by host order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

#
# Skia CQ Watcher
#

[[rule]]
name = "Too many CLs in CQ"
message = "There are 10 CLs or more in Skia's CL. https://skia.googlesource.com/buildbot/+/master/cq_watcher/PROD.md#too_many_cls"
database = "skmetrics"
query = "select max(value) from \"cq_watcher.in_flight.waiting_in_cq\" where time > now() - 5m and host='skia-cq-watcher' and app='cq_watcher'"
category = "infra"
conditions = ["x >= 10"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "CQ trybot running for too long"
message = "%(trybot)s in https://skia-review.googlesource.com/c/%(issue)s/%(patchset)s ran longer than 45 mins. https://skia.googlesource.com/buildbot/+/master/cq_watcher/PROD.md#trybot_duration_beyond_threshold"
database = "skmetrics"
query = "select max(value) from \"cq_watcher.in_flight.trybot_duration\" where time > now() - 5m and host='skia-cq-watcher' and app='cq_watcher' group by issue, patchset, trybot"
category = "infra"
conditions = ["x >= 45 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "Too many CQ trybots triggered by CL"
message = "There are more than 35 CQ trybots triggered by https://skia-review.googlesource.com/c/%(issue)s/%(patchset)s. https://skia.googlesource.com/buildbot/+/master/cq_watcher/PROD.md#too_many_trybots_triggered"
database = "skmetrics"
query = "select max(value) from \"cq_watcher.in_flight.trybot_num\" where time > now() - 5m and host='skia-cq-watcher' and app='cq_watcher' group by issue,patchset"
category = "infra"
conditions = ["x > 35"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

#
# Skia Prober
#

[[rule]]
name = "Probe Failure (probes)"
message = "Probing has failed to run in at least 10 minutes."
database = "skmetrics"
query = "select value from \"liveness\" where time > now() - 10m AND \"name\" = 'probes' AND app='probeserver' AND host='skia-monitoring' order by time desc limit 1"
category = "infra"
conditions = ["x >= 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Probe Failure (issue-tracker)"
message = "Ingesting issue tracker issued has failed to run in at least 30 minutes."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\" = 'issue-tracker' AND app='probeserver' AND host='skia-monitoring'"
category = "infra"
conditions = ["x >= 1800"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"


#
# Datahopper
#

[[rule]]
name = "Datahopper Error Rate"
message = "The error rate for datahopper is too high. http://104.154.112.122:10115/file_server/datahopper.ERROR"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='datahopper' AND level='ERROR' AND host='skia-datahopper2' order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"


#
# Swarming Bots
#

[[rule]]
name = "Swarming bot %(bot)s is missing"
message = """%(bot)s was last seen more than 15 minutes ago.

Bot: https://chromium-swarm.appspot.com/bot?id=%(bot)s
Howto: https://goto.google.com/skolo-maintenance"""
database = "skmetrics"
query = "select mean(value) from \"swarming.bots.last-seen\" where time > now() - 5m and app='datahopper' and host='skia-datahopper2' group by bot"
category = "infra"
conditions = ["x > 15 * 60 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Swarming bot %(bot)s is quarantined"
message = """%(bot)s has been quarantined for the last 15 minutes.

Bot: https://chromium-swarm.appspot.com/bot?id=%(bot)s
Howto: https://goto.google.com/skolo-maintenance"""
database = "skmetrics"
query = "select mean(value) from \"swarming.bots.quarantined\" where time > now() - 15m and app='datahopper' and host='skia-datahopper2' group by bot"
category = "infra"
conditions = ["x == 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# CTFE
#

[[rule]]
name = "CTFE Pending Task Count"
message = "There are a lot of pending tasks. https://skia.googlesource.com/buildbot/+/master/ct/PROD.md#ctfe_pending_tasks"
database = "skmetrics"
query = "select mean(value) from \"num-pending-tasks\" where time > now() - 10m AND app='ctfe' AND host='skia-ctfe'"
category = "infra"
conditions = ["x >= 20"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "CTFE Pending Task Status"
message = "A task has been waiting to be executed for a while and it's still not started. https://skia.googlesource.com/buildbot/+/master/ct/PROD.md#ctfe_pending_tasks"
database = "skmetrics"
query = "select mean(value) from \"oldest-pending-task-status\" where time > now() - 10m AND app='ctfe' AND host='skia-ctfe'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "CTFE Error Rate"
message = "The error rate for CTFE is too high. http://104.154.112.110:10115/ctfe.ERROR"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='ctfe' AND level='ERROR' AND host='skia-ctfe' order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "CT Poller Health Check"
message = "CT poller health check is failing. https://skia.googlesource.com/buildbot/+/master/ct/PROD.md#ct_poller_health_check"
database = "skmetrics"
query = "select mean(value) from \"healthy\" where time > now() - 10m AND app='ct-poller' AND host='skia-ct-master'"
category = "infra"
conditions = ["x < 0.01"]
actions = ["Email(infra-alerts@skia.org)", "Email(rmistry@google.com)"]
auto-dismiss = true
nag = "1h"

#
# datahopper_internal (internal.skia.org)
#

[[rule]]
name = "datahopper_internal Go routines"
message = "Too many Go routines."
database = "skmetrics"
query = "select mean(value) from \"runtime-metrics\" where time > now() - 10m AND \"metric\" = 'num-goroutine' AND app='datahopper_internal' AND  host='skia-internal'"
category = "infra"
conditions = ["x >= 100"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "datahopper_internal ingestBuild metrics"
message = "datahopper_internal goroutine for updateWebhookMetrics is dead or failing."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\" = 'ingest-build-webhook-oldest-untested-commit-age-metric' AND app='datahopper_internal' AND host='skia-internal'"
category = "infra"
conditions = ["x >= 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "datahopper_internal Google3-Autoroller up-to-date"
message = "Commit has not been picked up by Google3-Autoroller for over three hours. https://sites.google.com/a/google.com/skia-infrastructure/docs/google3-autoroller"
database = "skmetrics"
query = "select mean(value) from \"datahopper_internal.ingest-build-webhook.oldest-untested-commit-age\" where time > now() - 10m AND app='datahopper_internal' AND host='skia-internal' AND codename='Google3-Autoroller'"
category = "infra"
conditions = ["x >= 10800"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"


#
# Fuzzer
#
[[rule]]
name = "fuzzer upload queue"
message = "Fuzzer upload queue has been very full on %(host)s. https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#full_upload"
database = "skmetrics"
query = "SELECT max(value) from \"fuzzer.queue-size.upload\" WHERE time > now() - 10m AND app='fuzzer-be' group by host"
category = "infra"
conditions = ["x >= 90"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Fuzzer analysis queue"
message = "Fuzzer analysis queue has been very full on %(host)s. https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#full_analysis"
database = "skmetrics"
query = "SELECT max(value) from \"fuzzer.queue-size.analysis\" WHERE time > now() - 10m AND app='fuzzer-be' group by host"
category = "infra"
conditions = ["x >= 9000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "non-useful fuzzer"
message = "The %(category)s generator hasn't made much progress fuzzing on %(host)s. Perhaps it is misconfigured? https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#useless_fuzzer"
database = "skmetrics"
query = "SELECT mean(value) from \"fuzzer.stats.paths-total\" WHERE time > now() - 1h AND app='fuzzer-be' GROUP BY category,host"
category = "infra"
conditions = ["x < 20"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Stale Fuzzer version"
message = "The Fuzzer hasn't rolled its version forward in 10 days.  Roll it forward on fuzzer.skia.org https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#stale_version"
database = "skmetrics"
query = "SELECT mean(value) from \"fuzzer.version.age\" WHERE time > now() - 10m AND app='fuzzer-fe' AND host='skia-fuzzer-fe' AND type='current'"
category = "infra"
conditions = ["x > 10 * 24 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Fuzzer is taking a long time to roll forward"
message = "The fuzzer hasn't finished rolling its version forward in 2 hours.  Something might be wrong.  https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#broken_roll"
database = "skmetrics"
query = "SELECT mean(value) from \"fuzzer.version.age\" WHERE time > now() - 10m AND app='fuzzer-fe' AND host='skia-fuzzer-fe' AND type='pending'"
category = "infra"
conditions = ["x > 2 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Skolo
#
[[rule]]
name = "skia-rpi-master-spare has been active for more than 10 minutes"
message = "skia-rpi-master-spare has been active for more than 10 minutes. Something is probably wrong with skia-rpi-master.  go/skolo-maintenance"
database = "skmetrics"
query = "SELECT max(value) FROM \"skolo.hotspare.consecutive_failures\" WHERE time > now() - 10m AND app='hotspare'"
category = "infra"
conditions = ["x > 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "The rpi image backup has not been backed up in some time"
message = "The rpi image backup has not been backed up in at least 25 hours.  This should happen every day, but it hasn't.  go/skolo-maintenance"
database = "skmetrics"
query = "SELECT max(value) FROM \"skolo.rpi-backup.backup-size\" WHERE time > now() - 25h AND app='rpi-backup'"
category = "infra"
conditions = ["x < 100*1024*1024"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "%(app)s on %(host)s is down or crashing a lot."
message = "The skolo app %(app)s on %(host)s is down or is crashing a lot.  The average uptime for the last 20 minutes is below 10 minutes. go/skolo-maintenance"
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 20m AND host =~ /skia-rpi/ group by app, host"
category = "infra"
conditions = ["x < 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Task Scheduler
#

[[rule]]
name = "Task Scheduler Failing (%(host)s)"
message = "The task scheduler has failed to schedule for the last 10 minutes. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#scheduling_failed"
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='task_scheduler' and \"name\"='last-successful-task-scheduling' group by app, host"
category = "infra"
conditions = ["x > 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Task Scheduler HTTP Latency"
message = "https://task-scheduler.skia.org took more than 300ms to respond. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#http_latency"
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename='task_scheduler'"
category = "infra"
conditions = ["x / 1000000 > 300"]
actions = ["Email(infra-alerts@skia.org)", "Email(borenet@google.com)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Task Scheduler Error Rate (%(host)s)"
message = "The error rate for task_scheduler on %(host)s is too high. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#error_rate"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='task_scheduler' AND level='ERROR' group by host order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "Task Scheduler DB backup (%(host)s)"
message = "The last Task Scheduler DB backup was more than 25 hours ago. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#old_db_backup"
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 1h AND \"database\" = 'task_scheduler_db' AND \"name\" = 'last-db-backup' group by host"
category = "infra"
conditions = ["x > 25 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler extra DB backups (%(host)s)"
message = "There are too many recent Task Scheduler DB backups. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#too_many_recent_db_backups"
database = "skmetrics"
query = "select mean(value) from \"recent-db-backup-count\" where time > now() - 1h AND app='task_scheduler' group by host"
category = "infra"
conditions = ["x > 9"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler DB backup trigger (%(host)s)"
message = "The last time we checked for a Task Scheduler DB backup trigger file was more than 10 minutes ago. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#db_backup_trigger_liveness"
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 5m AND \"database\" = 'task_scheduler_db' AND \"name\" = 'db-backup-maybe-backup-db' group by host"
category = "infra"
conditions = ["x > 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler incremental backup (%(host)s)"
message = "The last time a Task Scheduler incremental backup succeeded was more than 10 minutes ago. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#incremental_backup_liveness"
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 5m AND \"database\" = 'task_scheduler_db' AND \"name\" = 'incremental-backup' group by host"
category = "infra"
conditions = ["x > 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler incremental backup reset (%(host)s)"
message = "Task Scheduler modified job tracking for incremental backups has been reset since last full backup. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#incremental_backup_reset"
database = "skmetrics"
query = "select mean(value) from \"counter\" where time > now() - 5m AND \"database\" = 'task_scheduler_db' AND \"name\" = 'incremental-backup-reset' group by host"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler DB excess free pages (%(host)s)"
message = "There are a large number of free pages in the Task Scheduler DB. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#db_too_many_free_pages"
database = "skmetrics"
query = "select mean(value) from \"db\" where time > now() - 1h AND \"database\" = 'task_scheduler_db' AND metric='FreePageCount' group by host"
category = "infra"
conditions = ["x > 150"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler Nightly Trigger (%(host)s)"
message = "The Task Scheduler's nightly trigger has not run in over 25 hours. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#trigger_nightly"
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 1h AND \"name\" = 'task-scheduler-periodic-trigger' AND trigger='nightly' group by host"
category = "infra"
conditions = ["x > 25 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

[[rule]]
name = "Task Scheduler Weekly Trigger (%(host)s)"
message = "The Task Scheduler's weekly trigger has not run in over 8 days. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#trigger_weekly"
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 1h AND \"name\" = 'task-scheduler-periodic-trigger' AND trigger='weekly' group by host"
category = "infra"
conditions = ["x > 8 * 24 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

