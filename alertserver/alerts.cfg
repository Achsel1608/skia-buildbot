# This file defines alerts to be triggered by the server.

#
# AlertServer should tolerate no errors.


[[rule]]
name = "AlertServer Errors"
message = "The Error rate for the alertserver is too high, please check the logs."
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='alertserver' AND level='ERROR' AND host='skia-alerts' order by time desc limit 1"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"


#
# General
#

[[rule]]
name = "Too many Go routines in %(app)s"
message = "Too many Go routines in %(app)s running on %(host)s"
database = "skmetrics"
query = "SELECT mean(value) FROM \"runtime-metrics\" WHERE metric='num-goroutine' AND time > now() - 10m GROUP BY app,host"
category = "infra"
conditions = ["x > 3000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Root Disk Space on %(host)s"
message = "Free space has fallen below 1GB on %(host)s root drive."
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE resource = 'df-root' AND host !~ /skia-rpi-.+/ AND time > now() - 5m GROUP BY host"
category = "infra"
conditions = ["x <= 1e9"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Disk Space on %(host)s"
message = "Free space has fallen below 10GB on %(host)s drive %(resource)s."
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE time > now() - 5m AND resource = 'df-mnt-pd0' OR resource = 'df-mnt-ssd0' GROUP BY host,resource"
category = "infra"
conditions = ["x <= 1e10"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Disk Space on VM %(host)s"
message = "Free space has fallen below 20GB on %(host)s drive %(resource)s.\n\nTry running:\ngo run scripts/run_on_swarming_bots/run_on_swarming_bots.go --alsologtostderr --script=scripts/run_on_swarming_bots/delete_out_dirs.py --dimension id:%(host)s"
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE time > now() - 5m AND resource = 'df-b' AND host !~ /skia-rpi-\\d+/ GROUP BY host,resource"
category = "infra"
conditions = ["x <= 2e10"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Low Disk Space on Skolo host %(host)s"
message = "Free space has fallen below 100MB on %(host)s drive %(resource)s."
database = "graphite"
query = "SELECT mean(value) FROM \"df_complex-free\" WHERE time > now() - 5m AND host =~ /skia-rpi-.+/ AND resource = 'df-var' OR resource = 'df-tmp' GROUP BY host,resource"
category = "infra"
conditions = ["x <= 1e8"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Probe failed %(probename)s"
message = "Endpoint %(probename)s has failed to respond in at least 10 minutes. See https://github.com/google/skia-buildbot/search?q=%(probename)s+filename%3Aprobers.json for the endpoint URL."
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='failure' group by probename;"
category = "infra"
conditions = ["x >= 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Instance %(host)s needs rebooting.  Owner(s): %(owners)s"
message = "See https://mon.skia.org/dashboard/db/reboots-required for the full list of instances that need rebooting."
database = "skmetrics"
query = "select mean(value) from \"reboot-required-i\" where time > now() - 5m AND app='pulld' group by host,owners;"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Skolo instance %(host)s needs rebooting.  Owner(s): kjlubick, stephana"
message = "See https://mon.skia.org/dashboard/db/reboots-required for the full list of instances that need rebooting."
database = "skmetrics"
query = "select mean(value) from \"reboot-required-i\" where time > now() - 5m AND app='pulld-not-gce' group by host;"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# SkiaPerf and SkiaGold
#

[[rule]]
name = "Perf Alerts"
message = "At least one perf alert has been found. Please visit https://perf.skia.org/alerts/ to triage."
database = "skmetrics"
query = "select value from \"perf.clustering.untriaged\" WHERE time > now() - 10m AND app = 'skiaperf' AND host = 'skia-perf' order by time desc limit 1"
category = "Perf"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"

[[rule]]
name = "Gold Alert (GM)"
message = "At least one untriaged GM has been found. Please visit https://gold.skia.org/ to triage."
database = "skmetrics"
query = "select value from \"gold.status.by-corpus\" WHERE time > now() - 10m AND type='untriaged' AND app = 'skiacorrectness' AND host = 'skia-gold-prod' order by time desc limit 1"
category = "Gold"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"

[[rule]]
name = "Expired Ignores (Gold)"
message = "At least one expired ignore rule has been found. Please visit https://gold.skia.org/ignores to delete or extend."
database = "skmetrics"
query = "select value from \"gold.num-expired-ignore-rules\" WHERE time > now() - 10m AND app = 'skiacorrectness' AND host = 'skia-gold-prod' order by time desc limit 1"
category = "Gold"
conditions = ["x > 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "24h"

[[rule]]
name = "Ingestion Failure (Gold)"
message = """At least two rounds of gold ingestion have failed back to back.

See: https://mon.skia.org/dashboard/db/ingestion
"""
database = "skmetrics"
query = "SELECT mean(value) FROM liveness WHERE time > now() - 10m AND app='gold_ingestion' AND \"name\"='gold' AND source='poll' AND metric='since-last-run'"
category = "infra"
conditions = ["x >= 750"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Ignore Monitoring Failure (Gold)"
message = "At least two rounds of monitoring for expired ignore rules have failed back to back."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\"='gold.expired-ignore-rules-monitoring' AND app='skiacorrectness' AND host='skia-gold-prod'"
category = "infra"
conditions = ["x >= 200"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Gold Error Rate"
message = "The error rate for Gold is too high."
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" WHERE time > now() - 10m AND \"name\"='skiacorrectness' AND level='ERROR' AND host='skia-gold-prod' order by time desc limit 1"
category = "infra"
conditions = ["x >= 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "Gold Ingestion Error Rate"
message = """The error rate for Gold Ingestion is too high.

See: https://mon.skia.org/dashboard/db/ingestion
"""
database = "skmetrics"
query = "SELECT  MAX(value) FROM ingestion WHERE time > now() - 10m AND ingester='gold' AND \"metric\"='errors' AND source='poll'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Gold Trybot Ingestion Error Rate"
message = """The error rate for Gold Trybot Ingestion is too high.

See: https://mon.skia.org/dashboard/db/ingestion
"""
database = "skmetrics"
query = "SELECT  MAX(value) FROM ingestion WHERE time > now() - 10m AND ingester='gold-trybot' AND \"metric\"='errors' AND source='poll'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "PDF Ingestion Error Rate"
message = "The error rate for PDF Ingestion is too high."
database = "skmetrics"
query = "SELECT  MAX(value) FROM ingestion WHERE time > now() - 30m AND ingester='pdf' AND \"metric\"='errors' AND source='poll'"
category = "infra"
conditions = ["x >= 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

#
# Skia Status
#

[[rule]]
name = "Skia Status Latency (JSON endpoint)"
message = "The JSON endpoint at https://status.skia.org/json/skia/commits/ took more than 300ms to respond."
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename='skiastatus_json'"
category = "infra"
conditions = ["x / 1000000 > 300"]
actions = ["Email(infra-alerts@skia.org)", "Email(borenet@google.com)"]
auto-dismiss = true
nag = "1h"

#
# Skia Alerts
#

[[rule]]
name = "Skia Alerts Latency (JSON endpoint)"
message = "The JSON endpoint at https://alerts.skia.org/json/alerts/ took more than 200ms to respond."
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename='skiaalerts_json'"
category = "infra"
conditions = ["x / 1000000 > 200"]
actions = ["Email(infra-alerts@skia.org)", "Email(borenet@google.com)"]
auto-dismiss = true
nag = "1h"

#
# Skia AutoRoll Bot
#

[[rule]]
name = "Skia AutoRoll Failed"
message = "The last DEPS roll attempt for Skia failed. https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#autoroll_failed"
database = "skmetrics"
query = "select value from \"autoroll.last-roll-result\" where time > now() - 10m AND \"child-path\"='src/third_party/skia' order by time desc limit 1"
category = "Skia"
conditions = ["x == 0"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "2h"

[[rule]]
name = "No AutoRolls Landed for 24h (Skia)"
message = "The last-landed AutoRoll for %(child-path)s was over 24h ago. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#no_rolls_24h"
database = "skmetrics"
query = "select value from \"liveness\" where time > now() - 10m AND \"name\"='last-autoroll-landed' AND \"child-path\"='src/third_party/skia' order by time desc limit 1"
category = "Skia"
conditions = ["x > 24 * 60 * 60"]
actions = ["Email(alerts@skia.org)"]
auto-dismiss = true
nag = "2h"

[[rule]]
name = "AutoRoll HTTP Latency (%(probename)s)"
message = "The endpoint for %(probename)s took more than 200ms to respond. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#http_latency"
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename =~ /autoroll/ group by probename"
category = "infra"
conditions = ["x / 1000000 > 200"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "AutoRoll Error Rate (%(host)s)"
message = "The error rate for autoroll on %(host)s is too high. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#error_rate"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='autoroll' AND level='ERROR' group by host order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

#
# Catapult AutoRoll
#
# These alerts are not sent to alerts@skia.org. Contact
# catapult-sheriff@chromium.org when triggered.
#

[[rule]]
name = "Catapult AutoRoll Failed"
message = "The last DEPS roll attempt for Catapult failed. https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#autoroll_failed. Skia devs: contact catapult-sheriff@chromium.org if this alert fires."
database = "skmetrics"
query = "select value from \"autoroll.last-roll-result\" where time > now() - 10m AND \"child-path\"='src/third_party/catapult' order by time desc limit 1"
category = "Skia"
conditions = ["x == 0"]
actions = ["Email(catapult-sheriff@chromium.org)"]
auto-dismiss = true
nag = "2h"

[[rule]]
name = "No AutoRolls Landed for 24h (Catapult)"
message = "The last-landed AutoRoll for %(child-path)s was over 24h ago. https://skia.googlesource.com/buildbot/+/master/autoroll/PROD.md#no_rolls_24h. Skia devs: contact catapult-sheriff@chromium.org if this alert fires."
database = "skmetrics"
query = "select value from \"liveness\" where time > now() - 10m AND \"name\"='last-autoroll-landed' AND \"child-path\"='src/third_party/catapult' order by time desc limit 1"
category = "Skia"
conditions = ["x > 24 * 60 * 60"]
actions = ["Email(catapult-sheriff@chromium.org)"]
auto-dismiss = true
nag = "2h"

#
# Skia Prober
#

[[rule]]
name = "Probe Failure (probes)"
message = "Probing has failed to run in at least 10 minutes."
database = "skmetrics"
query = "select value from \"liveness\" where time > now() - 10m AND \"name\" = 'probes' AND app='probeserver' AND host='skia-monitoring' order by time desc limit 1"
category = "infra"
conditions = ["x >= 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Probe Failure (issue-tracker)"
message = "Ingesting issue tracker issued has failed to run in at least 30 minutes."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\" = 'issue-tracker' AND app='probeserver' AND host='skia-monitoring'"
category = "infra"
conditions = ["x >= 1800"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"


#
# Datahopper
#

[[rule]]
name = "Datahopper Buildbot Ingestion Stalled (client.skia)"
message = "Buildbot Ingestion in Datahopper has failed to run in at least 5 minutes for client.skia."
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='datahopper' AND host='skia-datahopper2' AND \"name\"='buildbot-ingest' AND master='client.skia'"
category = "infra"
conditions = ["x >= 150"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Datahopper Buildbot Ingestion Stalled (client.skia.android)"
message = "Buildbot Ingestion in Datahopper has failed to run in at least 5 minutes for client.skia.android."
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='datahopper' AND host='skia-datahopper2' AND \"name\"='buildbot-ingest' AND master='client.skia.android'"
category = "infra"
conditions = ["x >= 150"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Datahopper Buildbot Ingestion Stalled (client.skia.compile)"
message = "Buildbot Ingestion in Datahopper has failed to run in at least 5 minutes for client.skia.compile."
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='datahopper' AND host='skia-datahopper2' AND \"name\"='buildbot-ingest' AND master='client.skia.compile'"
category = "infra"
conditions = ["x >= 150"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Datahopper Buildbot Ingestion Stalled (client.skia.fyi)"
message = "Buildbot Ingestion in Datahopper has failed to run in at least 5 minutes for client.skia.fyi."
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='datahopper' AND host='skia-datahopper2' AND \"name\"='buildbot-ingest' AND master='client.skia.fyi'"
category = "infra"
conditions = ["x >= 150"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Datahopper Buildbot Ingestion Too Slow"
message = "Buildbot ingestion time is more than 2 minutes."
database = "skmetrics"
query = "select mean(value) from timer where time > now() - 5m AND \"name\"='func-timer' and package='go.skia.org/infra/go/buildbot' and func='ingestNewBuilds' and app='datahopper' AND host='skia-datahopper2'"
category = "infra"
conditions = ["x >= 120 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Datahopper Buildbot Start to Ingestion Latency Too High"
message = "Average time between build start and first ingestion is more than 1 minute."
database = "skmetrics"
query = "select mean(value) from \"buildbot.ingest.latency\" where time > now() - 12h AND app='datahopper' AND host='skia-datahopper2' AND master!='client.skia.fake_internal'"
category = "infra"
conditions = ["x >= 60 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "3h"

[[rule]]
name = "Datahopper Error Rate"
message = "The error rate for datahopper is too high. http://104.154.112.122:10115/file_server/datahopper.ERROR"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='datahopper' AND level='ERROR' AND host='skia-datahopper2' order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "Buildbot DB backup"
message = "The last buildbot DB backup was more than 25 hours ago."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 1h AND app='datahopper' AND host='skia-datahopper2' AND \"name\" = 'last-buildbot-db-backup'"
category = "infra"
conditions = ["x > 25 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "6h"

#
# Buildbots
#

[[rule]]
name = "Buildslave %(buildslave)s Disconnected"
message = """Buildslave %(buildslave)s is not connected to %(master)s

Link: https://uberchromegw.corp.google.com/i/%(master)s/buildslaves/%(buildslave)s
Host info: https://status.skia.org/hosts?filter=%(buildslave)s
"""
database = "skmetrics"
query = "select mean(value) from \"buildbot.buildslaves.connected\" where time > now() - 10m AND host='skia-datahopper2' AND app='datahopper' group by buildslave,master"
category = "infra"
conditions = ["x < 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Possibly hung buildslave (%(buildslave)s)"
message = """Possibly hung buildslave (%(buildslave)s)

Step "%(step)s" has been running for over 3h:
https://uberchromegw.corp.google.com/i/%(master)s/builders/%(builder)s/builds/%(number)s
Dashboard: https://status.skia.org/buildbots?botGrouping=buildslave&filterBy=buildslave&include=%%5E%(buildslave)s%%24&tab=builds
Host info: https://status.skia.org/hosts?filter=%(buildslave)s"""
database = "skmetrics"
query = "select mean(value) from \"buildbot.buildsteps.running-time\" where time > now() - 2m AND builder != 'Housekeeper-Nightly-RecreateSKPs_Canary' AND builder != 'Housekeeper-Weekly-RecreateSKPs' AND builder != 'Linux Builder' AND builder != 'Mac Builder' AND builder != 'Test-Ubuntu-GCC-GCE-CPU-AVX2-x86_64-Release-Valgrind' AND builder != 'Test-Ubuntu-GCC-ShuttleA-GPU-GTX550Ti-x86_64-Release-Valgrind' AND builder != 'Perf-Ubuntu-GCC-ShuttleA-GPU-GTX550Ti-x86_64-Release-Valgrind' AND builder != 'Perf-Ubuntu-Clang-GCE-CPU-AVX2-x86_64-Debug-MSAN' AND builder != 'Win Builder' AND step != 'steps' AND step !~ /wait for device/ group by builder,master,number,buildslave,step"
empty-results-ok = true
category = "infra"
conditions = ["x > 3 * 60 * 60 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "The Android device for %(buildslave)s appears to be disconnected."
message = """The Android device for %(buildslave)s appears to be disconnected.

Build: https://uberchromegw.corp.google.com/i/%(master)s/builders/%(builder)s/builds/%(number)s
Dashboard: https://status.skia.org/buildbots?botGrouping=buildslave&filterBy=buildslave&include=%%5E%(buildslave)s%%24&tab=builds
Host info: https://status.skia.org/hosts?filter=%(buildslave)s"""
database = "skmetrics"
query = "select mean(value) from \"buildbot.buildsteps.running-time\" where time > now() - 2m AND step != 'steps' AND step =~ /wait for device/ group by builder,master,number,buildslave,step"
empty-results-ok = true
category = "infra"
conditions = ["x > 30 * 60 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "The Android device for %(buildslave)s has been charging a long time."
message = """The Android device for %(buildslave)s has been charging for longer than 3 hours.  Perhaps it has been disconnected or needs a new power hub?

Build: https://uberchromegw.corp.google.com/i/%(master)s/builders/%(builder)s/builds/%(number)s
Dashboard: https://status.skia.org/buildbots?botGrouping=buildslave&filterBy=buildslave&include=%%5E%(buildslave)s%%24&tab=builds
Host info: https://status.skia.org/hosts?filter=%(buildslave)s"""
database = "skmetrics"
query = "select mean(value) from \"buildbot.buildsteps.running-time\" where time > now() - 2m AND step != 'steps' AND step =~ /wait for charge/ group by builder,master,number,buildslave,step"
empty-results-ok = true
category = "infra"
conditions = ["x > 180 * 60 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

# This rule ensures that the above three queries with "empty-results-ok = true"
# are not passing just because there is no data coming in.  This typically
# indicates the data pipeline is broken.
[[rule]]
name = "No data for buildbot.buildsteps.running-time alerts"
message = "No data for buildbot.buildsteps.running-time alerts. This typically indicates the data pipeline is broken."
database = "skmetrics"
query = "select count(value) from \"buildbot.buildsteps.running-time\" where time > now() - 1h"
category = "infra"
conditions = ["x == 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Swarming Bots
#

[[rule]]
name = "Swarming bot %(bot)s is missing"
message = """%(bot)s was last seen more than 15 minutes ago.

Bot: https://chromium-swarm.appspot.com/newui/bot?id=%(bot)s https://chromium-swarm.appspot.com/restricted/bot/%(bot)s
Howto: https://goto.google.com/skolo-maintenance"""
database = "skmetrics"
query = "select mean(value) from \"buildbot.swarm-bots.last-seen\" where time > now() - 5m and app='datahopper' and host='skia-datahopper2' group by bot"
category = "infra"
conditions = ["x > 15 * 60 * 1000 * 1000 * 1000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Swarming bot %(bot)s is quarantined"
message = """%(bot)s has been quarantined for the last 15 minutes.

Bot: https://chromium-swarm.appspot.com/newui/bot?id=%(bot)s https://chromium-swarm.appspot.com/restricted/bot/%(bot)s
Howto: https://goto.google.com/skolo-maintenance"""
database = "skmetrics"
query = "select mean(value) from \"buildbot.swarm-bots.quarantined\" where time > now() - 15m and app='datahopper' and host='skia-datahopper2' group by bot"
category = "infra"
conditions = ["x == 1"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Buildbot Scheduler
#

[[rule]]
name = "Buildbot Scheduler Failing"
message = "The buildbot scheduler has failed to schedule builds for the last 10 minutes."
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='buildbot_scheduler' and host='skia-build-scheduler' and \"name\"='time-since-last-successful-scheduling' group by app, host"
category = "infra"
conditions = ["x > 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# CTFE
#

[[rule]]
name = "CTFE Pending Task Count"
message = "There are a lot of pending tasks. https://ct.skia.org/queue/"
database = "skmetrics"
query = "select mean(value) from \"num-pending-tasks\" where time > now() - 10m AND app='ctfe' AND host='skia-ctfe'"
category = "infra"
conditions = ["x >= 20"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "CTFE Pending Task Status"
message = "A task has been waiting to be executed for a while and it's still not started. https://ct.skia.org/queue/"
database = "skmetrics"
query = "select mean(value) from \"oldest-pending-task-status\" where time > now() - 10m AND app='ctfe' AND host='skia-ctfe'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "CTFE Error Rate"
message = "The error rate for CTFE is too high. http://104.154.112.110:10115/ctfe.ERROR"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='ctfe' AND level='ERROR' AND host='skia-ctfe' order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"

[[rule]]
name = "CT Poller Health Check"
message = "CT poller health check is failing. https://uberchromegw.corp.google.com/i/skia-ct-master/poller.ERROR?page_y=end"
database = "skmetrics"
query = "select mean(value) from \"healthy\" where time > now() - 10m AND app='ct-poller' AND host='build101-m5'"
category = "infra"
conditions = ["x < 0.01"]
actions = ["Email(infra-alerts@skia.org)", "Email(rmistry@google.com)"]
auto-dismiss = true
nag = "1h"

#
# datahopper_internal (internal.skia.org)
#

[[rule]]
name = "datahopper_internal liveness"
message = "Too long between successful gold Android ingestions."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\" = 'android-internal-ingest' AND app='datahopper_internal' AND host='skia-internal'"
category = "infra"
conditions = ["x >= 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "datahopper_internal Go routines"
message = "Too many Go routines."
database = "skmetrics"
query = "select mean(value) from \"runtime-metrics\" where time > now() - 10m AND \"metric\" = 'num-goroutine' AND app='datahopper_internal' AND  host='skia-internal'"
category = "infra"
conditions = ["x >= 100"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "datahopper_internal ingestBuild metrics"
message = "datahopper_internal goroutine for updateWebhookMetrics is dead or failing."
database = "skmetrics"
query = "select mean(value) from \"liveness\" where time > now() - 10m AND \"name\" = 'ingest-build-webhook-oldest-untested-commit-age-metric' AND app='datahopper_internal' AND host='skia-internal'"
category = "infra"
conditions = ["x >= 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "datahopper_internal Google3-Autoroller up-to-date"
message = "Commit has not been picked up by Google3-Autoroller for over three hours. https://sites.google.com/a/google.com/skia-infrastructure/docs/google3-autoroller"
database = "skmetrics"
query = "select mean(value) from \"datahopper_internal.ingest-build-webhook.oldest-untested-commit-age\" where time > now() - 10m AND app='datahopper_internal' AND host='skia-internal' AND codename='Google3-Autoroller'"
category = "infra"
conditions = ["x >= 10800"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"


#
# Fuzzer
#
[[rule]]
name = "fuzzer upload queue"
message = "Fuzzer upload queue has been very full. https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#full_upload"
database = "skmetrics"
query = "SELECT max(value) from \"fuzzer.queue-size.upload\" WHERE time > now() - 10m AND app='fuzzer-be' AND host='skia-fuzzer'"
category = "infra"
conditions = ["x >= 90"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Fuzzer analysis queue"
message = "Fuzzer analysis queue has been very full. https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#full_analysis"
database = "skmetrics"
query = "SELECT max(value) from \"fuzzer.queue-size.analysis\" WHERE time > now() - 10m AND app='fuzzer-be' AND host='skia-fuzzer'"
category = "infra"
conditions = ["x >= 9000"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "non-useful fuzzer"
message = "The %(category)s generator hasn't made much progress fuzzing. Perhaps it is misconfigured? https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#useless_fuzzer"
database = "skmetrics"
query = "SELECT mean(value) from \"fuzzer.stats.paths-total\" WHERE time > now() - 1h AND app='fuzzer-be' AND host='skia-fuzzer' GROUP BY category;"
category = "infra"
conditions = ["x < 20"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Stale Fuzzer version"
message = "The Fuzzer hasn't rolled its version forward in 10 days.  Roll it forward on fuzzer.skia.org https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#stale_version"
database = "skmetrics"
query = "SELECT mean(value) from \"fuzzer.version.age\" WHERE time > now() - 10m AND app='fuzzer-fe' AND host='skia-fuzzer' AND type='current'"
category = "infra"
conditions = ["x > 10 * 24 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Fuzzer is taking a long time to roll forward"
message = "The fuzzer hasn't finished rolling its version forward in 2 hours.  Something might be wrong.  https://skia.googlesource.com/buildbot/+/master/fuzzer/PROD.md#broken_roll"
database = "skmetrics"
query = "SELECT mean(value) from \"fuzzer.version.age\" WHERE time > now() - 10m AND app='fuzzer-fe' AND host='skia-fuzzer' AND type='pending'"
category = "infra"
conditions = ["x > 2 * 60 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Fiddle
#
[[rule]]
name = "fiddle is failing to build"
message = "Fiddle has failed to build at the last 2 chrome DEPS rolls. https://skia.googlesource.com/buildbot/+/master/fiddle/PROD.md#build_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='fiddle' AND \"name\"='builds-failed'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "fiddle is failing to sync"
message = "Fiddle has failed to sync to HEAD at least 2 times in a row. https://skia.googlesource.com/buildbot/+/master/fiddle/PROD.md#sync_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='fiddle' AND \"name\"='repo-sync-failed'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "There are named fiddles that fail to build."
message = "See https://fiddle.skia.org/f/ and https://skia.googlesource.com/buildbot/+/master/fiddle/PROD.md#named_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='fiddle' AND \"name\"='named-failures'"
category = "infra"
conditions = ["x > 0"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"


#
# ImageInfo
#
[[rule]]
name = "Imageinfo is failing to build"
message = "Imageinfo has failed to build at the last 2 chrome DEPS rolls. https://skia.googlesource.com/buildbot/+/master/imageinfo/PROD.md#build_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='imageinfo' AND \"name\"='builds-failed'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "ImageInfo is failing to sync"
message = "Imageinfo has failed to sync to HEAD at least 2 times in a row. https://skia.googlesource.com/buildbot/+/master/imageinfo/PROD.md#sync_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='imageinfo' AND \"name\"='repo-sync-failed'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Debugger
#
[[rule]]
name = "Debugger is failing to build"
message = "debugger has failed to build at the last 2 chrome DEPS rolls. https://skia.googlesource.com/buildbot/+/master/debugger/PROD.md#build_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='debugger' AND \"name\"='builds-failed'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Debugger is failing to sync"
message = "debugger has failed to sync to HEAD at least 2 times in a row. https://skia.googlesource.com/buildbot/+/master/debugger/PROD.md#sync_fail"
database = "skmetrics"
query = "SELECT mean(value) FROM \"counter\" WHERE time > now() - 10m AND app='debugger' AND \"name\"='repo-sync-failed'"
category = "infra"
conditions = ["x >= 2"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Skolo
#
[[rule]]
name = "skia-rpi-master-spare has been active for more than 10 minutes"
message = "skia-rpi-master-spare has been active for more than 10 minutes. Something is probably wrong with skia-rpi-master.  go/skolo-maintenance"
database = "skmetrics"
query = "SELECT max(value) FROM \"skolo.hotspare.consecutive_failures\" WHERE time > now() - 10m AND app='hotspare'"
category = "infra"
conditions = ["x > 600"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "The rpi image backup has not been backed up in some time"
message = "The rpi image backup has not been backed up in at least 25 hours.  This should happen every day, but it hasn't.  go/skolo-maintenance"
database = "skmetrics"
query = "SELECT max(value) FROM \"skolo.rpi-backup.backup-size\" WHERE time > now() - 25h AND app='rpi-backup'"
category = "infra"
conditions = ["x < 100*1024*1024"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "%(app)s on %(host)s is down or crashing a lot."
message = "The skolo app %(app)s on %(host)s is down or is crashing a lot.  The average uptime for the last 20 minutes is below 10 minutes. go/skolo-maintenance"
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 20m AND host =~ /skia-rpi/ group by app, host"
category = "infra"
conditions = ["x < 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

#
# Task Scheduler
#

[[rule]]
name = "Task Scheduler Failing"
message = "The task scheduler has failed to schedule for the last 10 minutes. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#scheduling_failed"
database = "skmetrics"
query = "select mean(value) from liveness where time > now() - 5m AND app='task_scheduler' and host='skia-task-scheduler' and \"name\"='last-successful-task-scheduling' group by app, host"
category = "infra"
conditions = ["x > 10 * 60"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Task Scheduler HTTP Latency"
message = "https://task-scheduler.skia.org took more than 300ms to respond. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#http_latency"
database = "skmetrics"
query = "select mean(value) from \"prober\" where time > now() - 10m AND type='latency' AND probename='task_scheduler'"
category = "infra"
conditions = ["x / 1000000 > 300"]
actions = ["Email(infra-alerts@skia.org)", "Email(borenet@google.com)"]
auto-dismiss = true
nag = "1h"

[[rule]]
name = "Task Scheduler Error Rate (%(host)s)"
message = "The error rate for task_scheduler on %(host)s is too high. https://skia.googlesource.com/buildbot/+/master/task_scheduler/PROD.md#error_rate"
database = "skmetrics"
query = "select derivative(value, 10m) from \"logs\" where time > now() - 10m AND \"name\"='task_scheduler' AND level='ERROR' group by host order by time desc limit 1"
category = "infra"
conditions = ["x >= 5"]
actions = ["Email(infra-alerts@skia.org)"]
auto-dismiss = false
nag = "1h"
